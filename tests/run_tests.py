#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
RQA2025 è‡ªåŠ¨åŒ–æµ‹è¯•æ‰§è¡Œä¸»ç¨‹åº
"""

import argparse
import concurrent.futures
from typing import List, Dict
import time
from datetime import datetime
from pathlib import Path
import sys

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from utils.test_environment import TestEnvironmentManager
from utils.test_monitor import TestMonitor
from utils.data_visualizer import TestDataVisualizer
from utils.report_generator import HTMLReportGenerator

class TestRunner:
    def __init__(self, config_path="config/test_config.json"):
        """
        åˆå§‹åŒ–æµ‹è¯•è¿è¡Œå™¨
        :param config_path: æµ‹è¯•é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = self._load_config(config_path)
        self.monitor = TestMonitor(self.config.get("alert"))
        self.visualizer = TestDataVisualizer(port=self.config.get("visualizer_port", 8050))
        self.report_generator = HTMLReportGenerator()

    def run_tests(self, test_types: List[str] = None, parallel: int = 1):
        """
        æ‰§è¡Œæµ‹è¯•
        :param test_types: è¦æ‰§è¡Œçš„æµ‹è¯•ç±»å‹åˆ—è¡¨(unit/integration/performance)
        :param parallel: å¹¶è¡Œæ‰§è¡Œæ•°
        """
        print("ğŸš€ å¯åŠ¨ RQA2025 è‡ªåŠ¨åŒ–æµ‹è¯•")
        start_time = datetime.now()

        # å¯åŠ¨å¯è§†åŒ–é¢æ¿
        self.visualizer.start()

        # åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ
        with TestEnvironmentManager(self.config["environment"]) as env:
            print("\nğŸ”§ æµ‹è¯•ç¯å¢ƒå‡†å¤‡å°±ç»ª")

            # ç¡®å®šè¦æ‰§è¡Œçš„æµ‹è¯•ç”¨ä¾‹
            test_cases = self._select_test_cases(test_types)
            print(f"ğŸ“‹ å…± {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹å¾…æ‰§è¡Œ")

            # æ‰§è¡Œæµ‹è¯•
            if parallel > 1:
                self._run_parallel_tests(test_cases, parallel)
            else:
                self._run_sequential_tests(test_cases)

        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        self._generate_reports(start_time)

        print(f"\nâœ… æ‰€æœ‰æµ‹è¯•æ‰§è¡Œå®Œæˆï¼Œæ€»è€—æ—¶: {datetime.now() - start_time}")

    def _run_sequential_tests(self, test_cases: List[Dict]):
        """é¡ºåºæ‰§è¡Œæµ‹è¯•ç”¨ä¾‹"""
        for case in test_cases:
            self._execute_test_case(case)

    def _run_parallel_tests(self, test_cases: List[Dict], max_workers: int):
        """å¹¶è¡Œæ‰§è¡Œæµ‹è¯•ç”¨ä¾‹"""
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = []
            for case in test_cases:
                futures.append(executor.submit(self._execute_test_case, case))

            # ç­‰å¾…æ‰€æœ‰æµ‹è¯•å®Œæˆ
            for future in concurrent.futures.as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    print(f"âŒ æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {str(e)}")

    def _execute_test_case(self, test_case: Dict):
        """æ‰§è¡Œå•ä¸ªæµ‹è¯•ç”¨ä¾‹"""
        # è®°å½•æµ‹è¯•å¼€å§‹
        self.monitor.start_test_case(test_case["name"], test_case["type"])
        print(f"\nâ–¶ï¸ å¼€å§‹æ‰§è¡Œ: {test_case['name']} ({test_case['type']})")

        try:
            # åŠ¨æ€å¯¼å…¥æµ‹è¯•æ¨¡å—
            module = __import__(f"tests.{test_case['module']}", fromlist=[test_case["class"]])
            test_class = getattr(module, test_case["class"])
            test_method = getattr(test_class(), test_case["method"])

            # æ‰§è¡Œæµ‹è¯•
            start_time = time.time()
            test_method()
            duration = time.time() - start_time

            # è®°å½•æµ‹è¯•é€šè¿‡
            self.monitor.end_test_case(
                test_case["name"],
                "passed",
                f"æ‰§è¡ŒæˆåŠŸï¼Œè€—æ—¶: {duration:.2f}ç§’"
            )
            print(f"âœ… æµ‹è¯•é€šè¿‡: {test_case['name']} (è€—æ—¶: {duration:.2f}ç§’)")

            # æ›´æ–°å¯è§†åŒ–æ•°æ®
            self._update_visualizer(test_case, duration)

        except AssertionError as e:
            # æ–­è¨€å¤±è´¥
            self.monitor.end_test_case(
                test_case["name"],
                "failed",
                f"æ–­è¨€å¤±è´¥: {str(e)}"
            )
            print(f"âŒ æµ‹è¯•å¤±è´¥: {test_case['name']} - {str(e)}")

        except Exception as e:
            # å…¶ä»–å¼‚å¸¸
            self.monitor.end_test_case(
                test_case["name"],
                "failed",
                f"æ‰§è¡Œå¼‚å¸¸: {str(e)}"
            )
            print(f"âš ï¸ æµ‹è¯•å¼‚å¸¸: {test_case['name']} - {str(e)}")

    def _update_visualizer(self, test_case: Dict, duration: float):
        """æ›´æ–°å¯è§†åŒ–æ•°æ®"""
        if test_case["type"] == "performance":
            # æ€§èƒ½æµ‹è¯•æ•°æ®
            self.visualizer.add_performance_data(
                latency=duration,
                throughput=1000/duration if duration > 0 else 0
            )
        else:
            # æ¨¡æ‹Ÿå¸‚åœºæ•°æ®
            symbol = test_case.get("symbol", "600519.SH")
            price = 1800 + (hash(test_case["name"]) % 200 - 100)
            volume = 10000 + (hash(test_case["name"]) % 5000)
            self.visualizer.add_market_data(symbol, price, volume)

            # æ¨¡æ‹Ÿè®¢å•æ•°æ®
            self.visualizer.add_order_data(
                symbol=symbol,
                price=price,
                quantity=100,
                status="FILLED"
            )

    def _select_test_cases(self, test_types: List[str] = None) -> List[Dict]:
        """é€‰æ‹©è¦æ‰§è¡Œçš„æµ‹è¯•ç”¨ä¾‹"""
        if test_types is None:
            return self.config["test_cases"]

        return [
            case for case in self.config["test_cases"]
            if case["type"] in test_types
        ]

    def _generate_reports(self, start_time):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆæµ‹è¯•æŠ¥å‘Šä¸­...")

        # å‡†å¤‡æŠ¥å‘Šæ•°æ®
        test_results = {
            "test_cases": [],
            "performance": {
                "start_time": start_time.isoformat(),
                "end_time": datetime.now().isoformat()
            }
        }

        # æ·»åŠ æµ‹è¯•ç”¨ä¾‹ç»“æœ
        for case in self.monitor.get_summary().to_dict("records"):
            test_results["test_cases"].append({
                "name": case["name"],
                "type": case["type"],
                "status": case["status"],
                "duration": case["duration"],
                "error": case["error"]
            })

        # ä¿å­˜å¹¶ç”ŸæˆæŠ¥å‘Š
        self.report_generator.save_test_results(test_results)
        report_path = self.report_generator.generate_report(test_results)
        print(f"ğŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ: file://{Path(report_path).absolute()}")

    def _load_config(self, config_path: str) -> Dict:
        """åŠ è½½æµ‹è¯•é…ç½®"""
        import json
        with open(config_path, "r", encoding="utf-8") as f:
            return json.load(f)


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(description="RQA2025 è‡ªåŠ¨åŒ–æµ‹è¯•æ‰§è¡Œç¨‹åº")
    parser.add_argument(
        "--types",
        nargs="+",
        choices=["unit", "integration", "performance"],
        help="æŒ‡å®šè¦æ‰§è¡Œçš„æµ‹è¯•ç±»å‹"
    )
    parser.add_argument(
        "--parallel",
        type=int,
        default=1,
        help="å¹¶è¡Œæ‰§è¡Œæ•°"
    )
    parser.add_argument(
        "--config",
        default="config/test_config.json",
        help="æµ‹è¯•é…ç½®æ–‡ä»¶è·¯å¾„"
    )

    args = parser.parse_args()

    try:
        runner = TestRunner(args.config)
        runner.run_tests(args.types, args.parallel)
    except Exception as e:
        print(f"âŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {str(e)}", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    main()
